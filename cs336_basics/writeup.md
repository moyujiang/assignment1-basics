Unicode1ï¼š

(a) ç©ºä¸² `""`ï¼›

(b) `__str__(chr(0))` ç»™å‡ºç©ºä¸²ï¼Œ`__repr__(chr(0))` ç»™å‡º `'\x00'`ï¼Œå³ç”¨å¼•å·åŒ…è£¹çš„ unicodeï¼›

(c) 

```py
>>> chr(0)
'\x00'
>>> print(chr(0))

>>> "this is a test" + chr(0) + "string"
'this is a test\x00string'
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```



Unicode2ï¼š

(a) å¹³å‡ä¿¡æ¯å¯†åº¦é«˜ï¼ˆç¼–ç å­—æ¯åªéœ€è¦ $1$ å­—èŠ‚ï¼‰ï¼Œç”¨ 00 å¡«å……çš„æƒ…å†µæ›´å°‘ï¼Œç¬¦åˆäº’è”ç½‘æ–‡æœ¬å­˜å‚¨æ ¼å¼ï¼›

(b) `b'\xf0\x9f\xa4\x93'`ï¼Œemoji ğŸ¤“ çš„ Unicodeã€‚UTF-8 æ˜¯å˜é•¿çš„ï¼Œä»»ä½•åœ¨ UTF-8 ä¸‹ç¼–ç è¶…è¿‡ä¸€å­—èŠ‚çš„éƒ½ä¼šåœ¨ `.decode` å¼€å¤´å­—èŠ‚çš„æ—¶å€™ç›´æ¥æŠ¥é”™ã€‚

(c) `b'\xf0\x9f`ï¼Œ`0xf0` è¯´æ˜æ¥ä¸‹æ¥è¿˜ä¼šæœ‰ 3 byte æ¥ä¸€èµ·ç¼–ç ä¸€ä¸ªå­—ç¬¦ï¼Œä½†æ¥ä¸‹æ¥åªæœ‰ 1 byte äº†ï¼Œè§£ç æ—¶æŠ¥é”™ã€‚



train_bpe_tinystoriesï¼š

(a) 

```
Time taken: 88.10 seconds (0.0245 hours)
Peak memory usage: 2.31 GB
Vocabulary size: 10000
Number of merges: 9743
Longest token: b' accomplishment' (15 bytes)
As string: ' accomplishment'
```

(b) åœ¨ valid-set ä¸Šï¼Œpretokenize èŠ±è´¹ 0.74sï¼Œtrain èŠ±è´¹ 3.67sï¼›åœ¨ train-set ä¸Šï¼Œpretokenize èŠ±è´¹ 74.03sï¼Œ train èŠ±è´¹ 13.57sã€‚



train_bpe_expts_owtï¼š

(a)

```
Time taken: 12932.71 seconds (3.5924 hours)
Pretokenize time: 525.41 seconds
Train time: 8944.50 seconds
Peak memory usage: 28.58 GB
Vocabulary size: 32000
Number of merges: 31743

Longest token: b'\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82' (64 bytes)
As string: 'ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚'
```

(b)

ç½‘ç»œæ•°æ®æ›´éš¾æ¸…æ´—ï¼Œå¯¼è‡´ owt ä¸Šçš„æœ€é•¿ token ä¸æ˜¯æœ‰æ„ä¹‰çš„å•è¯ï¼›owt æ•°æ®é›†æ˜¾è‘—å¤§äº TinyStoriesï¼Œè¿™å¯¼è‡´ merge é˜¶æ®µæ…¢å¾ˆå¤šã€‚



tokenizer_experimentsï¼š

(a)

TinyStories tokenizer (10K) compression ratio (bytes/token): 4.0908
OpenWebText tokenizer (32K) compression ratio (bytes/token): 4.5357

(b)

OpenWebText sample with TinyStories tokenizer (bytes/token): 3.3438
OWT token count multiplier (TinyStories vs OWT tokenizer): 1.356x
OWT bytes/token change vs OWT tokenizer: -26.28%

TinyStories ä¸­å¤šç®€çŸ­æ•…äº‹ï¼Œç”¨è¯ç›¸å¯¹ç®€å•ï¼Œä¸” vocab æ›´å°ï¼Œå¯¹ owt ä¸­çš„å°‘è§è¯æ±‡ã€URL æ¨¡å¼ç­‰ä¸èƒ½å¾ˆå¥½åœ°åŒ¹é…ï¼Œè¢«åˆ‡ä¸ºæ›´å¤šçš„ tokensï¼Œå‹ç¼©ç‡æ˜¾è‘—ä¸‹é™ã€‚

(c)

OWT tokenizer åœ¨ OWT ä¸Šçº¦ ~1.5 MB/sï¼Œç”¨è¿™ä¸ªååé‡ç²—ä¼° Pile 825GB æ—¶é—´ï¼š$t \approx \frac{825\times 10^9}{1.5\times 10^6}s\approx 6d$ï¼Œçº¦ $6$ å¤©ã€‚

(d)

```
[tinystories:train] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/tinystories_train.uint16.npy
  tokens=541,229,347  count=52.50s (42.43 MB/s)  write=123.90s (17.98 MB/s)  encode=4368132 tok/s
[tinystories:valid] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/tinystories_valid.uint16.npy
  tokens=5,465,883  count=1.60s (14.07 MB/s)  write=2.67s (8.43 MB/s)  encode=2047747 tok/s
[owt:train] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/owt_train.uint16.npy
  tokens=2,727,120,452  count=487.05s (24.47 MB/s)  write=1130.65s (10.54 MB/s)  encode=2411991 tok/s
[owt:valid] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/owt_valid.uint16.npy
  tokens=66,401,098  count=13.88s (20.89 MB/s)  write=30.31s (9.57 MB/s)  encode=2190728 tok/s
```

é€‰æ‹© uint16 æ˜¯å› ä¸º $[0,65535]$ çš„å€¼åŸŸè¦†ç›– 10K å’Œ 32K å¤§å°çš„ vocab ä¸‹çš„ token IDï¼ŒåŒæ—¶æ¯” uint32 èŠ‚çœä¸€åŠçš„ç©ºé—´ã€‚



transformer_accountingï¼š

(a)

$V$ è¡¨ç¤º vocab_sizeï¼Œ$L$ è¡¨ç¤º num_layersï¼Œ$h$ è¡¨ç¤º num_headsã€‚

å‚æ•°é‡ï¼š

- Embedding $Vd$ï¼›
- LM head $Vd$ï¼›
- æ¯å±‚ attn proj $4d_{model}^2$ï¼ˆ$W_Q,W_K,W_V,W_O$ï¼‰ï¼›
- æ¯å±‚ FFN $3d_{model}d_{ff}$ï¼›
- æ¯å±‚ RMSNorm $2d_{model}$ï¼›
- æœ€ç»ˆ RMSNorm $d_{model}$ï¼›

æ€»å‚æ•° $2Vd_{model}+L(4d_{model}^2+3d_{model}d_{ff}+2d_{model})+d_{model}$ï¼Œå¸¦å…¥ GPT-2 XL çš„æ•°æ®å¾—å‚æ•°é‡çº¦ $2.13B$ï¼Œå•ç²¾åº¦å­˜å‚¨çº¦ $8.5G$ã€‚

(b)

åœ¨ seq_len = 1024 çš„æƒ…å†µä¸‹ï¼ŒçŸ©ä¹˜åœ¨ä»¥ä¸‹åœºæ™¯å‡ºç°ï¼š

- æ¯å±‚ï¼ˆå…± 48 å±‚ï¼‰ï¼š
  1. QKV Projectionï¼š`(1024, 1600) @ (1600, 1600) -> (1024, 1600)` *3ï¼›
  2. Attn scoresï¼š`(25*1024, 64) @ (64, 1024) -> (25*1024, 1024)`ï¼›
  3. Attn weightedï¼š`(25*1024, 1024) @ (1024, 64) -> (25*1024, 64)`ï¼›
  4. Out Projï¼š`(1024, 1600) @ (1600, 1600) -> (1024, 1600)`ï¼›
  5. FFN w1/w3ï¼š`(1024, 1600) @ (1600, 6400) -> (1024, 6400)` *2ï¼›
  6. FFN w2ï¼š`(1024, 6400) @ (6400, 1600) -> (1024, 1600)`ï¼›
- LM headï¼š
  7. Outputï¼š`(1024, 1600) @ (1600, 50257) -> (1024, 50257)`ã€‚

Total FLOPs = 4.513T.

(c)

```
--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      754.975B
Attention scores:     161.061B
Attention weighted:   161.061B
Output projection:    251.658B
FFN (SwiGLU):           3.020T
LM head:              164.682B

Total FLOPs:            4.513T

--- FLOPs Proportions ---
       QKV proj: 16.73%
    Attn scores:  3.57%
  Attn weighted:  3.57%
       Out proj:  5.58%
            FFN: 66.91%
        LM head:  3.65%
```

å› æ­¤ FFN å æ®æœ€å¤šçš„ FLOPsã€‚

(d)

```
================================================================================
GPT-2 Small
================================================================================
Config: vocab=50257, ctx=1024, L=12, d=768, h=12, d_ff=3072

--- Parameters ---
Token embedding:       38.597M
Per layer:
  MHSA:                 2.359M
  FFN (SwiGLU):         7.078M
  RMSNorm (x2):         1.536K
  Layer total:          9.439M
All layers:           113.265M
Final RMSNorm:             768
LM head:               38.597M

Total parameters:     190.460M (190,460,160)
Memory (fp32):      0.7618 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:       43.487B
Attention scores:      19.327B
Attention weighted:    19.327B
Output projection:     14.496B
FFN (SwiGLU):         173.946B
LM head:               79.047B

Total FLOPs:          349.630B

--- FLOPs Proportions ---
       QKV proj: 12.44%
    Attn scores:  5.53%
  Attn weighted:  5.53%
       Out proj:  4.15%
            FFN: 49.75%
        LM head: 22.61%

================================================================================
GPT-2 Medium
================================================================================
Config: vocab=50257, ctx=1024, L=24, d=1024, h=16, d_ff=4096

--- Parameters ---
Token embedding:       51.463M
Per layer:
  MHSA:                 4.194M
  FFN (SwiGLU):        12.583M
  RMSNorm (x2):         2.048K
  Layer total:         16.779M
All layers:           402.702M
Final RMSNorm:          1.024K
LM head:               51.463M

Total parameters:     505.630M (505,629,696)
Memory (fp32):      2.0225 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      154.619B
Attention scores:      51.540B
Attention weighted:    51.540B
Output projection:     51.540B
FFN (SwiGLU):         618.475B
LM head:              105.397B

Total FLOPs:            1.033T

--- FLOPs Proportions ---
       QKV proj: 14.97%
    Attn scores:  4.99%
  Attn weighted:  4.99%
       Out proj:  4.99%
            FFN: 59.87%
        LM head: 10.20%

================================================================================
GPT-2 Large
================================================================================
Config: vocab=50257, ctx=1024, L=36, d=1280, h=20, d_ff=5120

--- Parameters ---
Token embedding:       64.329M
Per layer:
  MHSA:                 6.554M
  FFN (SwiGLU):        19.661M
  RMSNorm (x2):         2.560K
  Layer total:         26.217M
All layers:           943.811M
Final RMSNorm:          1.280K
LM head:               64.329M

Total parameters:       1.072B (1,072,469,760)
Memory (fp32):      4.2899 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      362.388B
Attention scores:      96.637B
Attention weighted:    96.637B
Output projection:    120.796B
FFN (SwiGLU):           1.450T
LM head:              131.746B

Total FLOPs:            2.258T

--- FLOPs Proportions ---
       QKV proj: 16.05%
    Attn scores:  4.28%
  Attn weighted:  4.28%
       Out proj:  5.35%
            FFN: 64.20%
        LM head:  5.84%
```

éšç€æ¨¡å‹å¤§å°æå‡ï¼ŒFFN FLOPs å æ¯”æå‡ï¼ŒLM head FLOPs å æ¯”ä¸‹é™ï¼Œå…¶ä½™éƒ¨åˆ†å˜åŒ–ä¸æ˜æ˜¾ã€‚

(e)

```
================================================================================
GPT-2 XL (16K context)
================================================================================
Config: vocab=50257, ctx=16384, L=48, d=1600, h=25, d_ff=6400

--- Parameters ---
Token embedding:       80.411M
Per layer:
  MHSA:                10.240M
  FFN (SwiGLU):        30.720M
  RMSNorm (x2):         3.200K
  Layer total:         40.963M
All layers:             1.966B
Final RMSNorm:          1.600K
LM head:               80.411M

Total parameters:       2.127B (2,127,057,600)
Memory (fp32):      8.5082 GB

--- FLOPs (batch=1, seq_len=16384) ---
QKV projections:       12.080T
Attention scores:      41.232T
Attention weighted:    41.232T
Output projection:      4.027T
FFN (SwiGLU):          48.318T
LM head:                2.635T

Total FLOPs:          149.523T

--- FLOPs Proportions ---
       QKV proj:  8.08%
    Attn scores: 27.58%
  Attn weighted: 27.58%
       Out proj:  2.69%
            FFN: 32.32%
        LM head:  1.76%
```

Attention FLOPs å æ¯”æå‡å¾ˆå¤§ï¼ŒFFN FLOPs å æ¯”é™ä½ï¼Œå‰©ä¸‹çš„éƒ¨åˆ†å æ¯”ä¹Ÿé™ä½ï¼Œå› ä¸ºéšç€æ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œattention éƒ¨åˆ†çš„è®¡ç®—é‡æ˜¯å¹³æ–¹å¢é•¿çš„ï¼Œå…¶å®ƒçš„åªæ˜¯çº¿æ€§å¢é•¿ã€‚

æ–‡æœ¬é•¿åº¦å¢åŠ  16 å€ï¼ŒFLOPs å¢é•¿ 33.13 å€ã€‚
