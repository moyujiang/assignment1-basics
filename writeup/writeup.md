## Unicode1

(a) ç©ºä¸² `""`ï¼›

(b) `__str__(chr(0))` ç»™å‡ºç©ºä¸²ï¼Œ`__repr__(chr(0))` ç»™å‡º `'\x00'`ï¼Œå³ç”¨å¼•å·åŒ…è£¹çš„ unicodeï¼›

(c) 

```py
>>> chr(0)
'\x00'
>>> print(chr(0))

>>> "this is a test" + chr(0) + "string"
'this is a test\x00string'
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```



## Unicode2

(a) å¹³å‡ä¿¡æ¯å¯†åº¦é«˜ï¼ˆç¼–ç å­—æ¯åªéœ€è¦ $1$ å­—èŠ‚ï¼‰ï¼Œç”¨ 00 å¡«å……çš„æƒ…å†µæ›´å°‘ï¼Œç¬¦åˆäº’è”ç½‘æ–‡æœ¬å­˜å‚¨æ ¼å¼ï¼›

(b) `b'\xf0\x9f\xa4\x93'`ï¼Œemoji ğŸ¤“ çš„ Unicodeã€‚UTF-8 æ˜¯å˜é•¿çš„ï¼Œä»»ä½•åœ¨ UTF-8 ä¸‹ç¼–ç è¶…è¿‡ä¸€å­—èŠ‚çš„éƒ½ä¼šåœ¨ `.decode` å¼€å¤´å­—èŠ‚çš„æ—¶å€™ç›´æ¥æŠ¥é”™ã€‚

(c) `b'\xf0\x9f`ï¼Œ`0xf0` è¯´æ˜æ¥ä¸‹æ¥è¿˜ä¼šæœ‰ 3 byte æ¥ä¸€èµ·ç¼–ç ä¸€ä¸ªå­—ç¬¦ï¼Œä½†æ¥ä¸‹æ¥åªæœ‰ 1 byte äº†ï¼Œè§£ç æ—¶æŠ¥é”™ã€‚



## train_bpe_tinystories

(a) 

```
Time taken: 88.10 seconds (0.0245 hours)
Peak memory usage: 2.31 GB
Vocabulary size: 10000
Number of merges: 9743
Longest token: b' accomplishment' (15 bytes)
As string: ' accomplishment'
```

(b) åœ¨ valid-set ä¸Šï¼Œpretokenize èŠ±è´¹ 0.74sï¼Œtrain èŠ±è´¹ 3.67sï¼›åœ¨ train-set ä¸Šï¼Œpretokenize èŠ±è´¹ 74.03sï¼Œ train èŠ±è´¹ 13.57sã€‚



## train_bpe_expts_owt

(a)

```
Time taken: 12932.71 seconds (3.5924 hours)
Pretokenize time: 525.41 seconds
Train time: 8944.50 seconds
Peak memory usage: 28.58 GB
Vocabulary size: 32000
Number of merges: 31743

Longest token: b'\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82' (64 bytes)
As string: 'ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚'
```

(b)

ç½‘ç»œæ•°æ®æ›´éš¾æ¸…æ´—ï¼Œå¯¼è‡´ owt ä¸Šçš„æœ€é•¿ token ä¸æ˜¯æœ‰æ„ä¹‰çš„å•è¯ï¼›owt æ•°æ®é›†æ˜¾è‘—å¤§äº TinyStoriesï¼Œè¿™å¯¼è‡´ merge é˜¶æ®µæ…¢å¾ˆå¤šã€‚



## tokenizer_experiments

(a)

TinyStories tokenizer (10K) compression ratio (bytes/token): 4.0908
OpenWebText tokenizer (32K) compression ratio (bytes/token): 4.5357

(b)

OpenWebText sample with TinyStories tokenizer (bytes/token): 3.3438
OWT token count multiplier (TinyStories vs OWT tokenizer): 1.356x
OWT bytes/token change vs OWT tokenizer: -26.28%

TinyStories ä¸­å¤šç®€çŸ­æ•…äº‹ï¼Œç”¨è¯ç›¸å¯¹ç®€å•ï¼Œä¸” vocab æ›´å°ï¼Œå¯¹ owt ä¸­çš„å°‘è§è¯æ±‡ã€URL æ¨¡å¼ç­‰ä¸èƒ½å¾ˆå¥½åœ°åŒ¹é…ï¼Œè¢«åˆ‡ä¸ºæ›´å¤šçš„ tokensï¼Œå‹ç¼©ç‡æ˜¾è‘—ä¸‹é™ã€‚

(c)

OWT tokenizer åœ¨ OWT ä¸Šçº¦ ~1.5 MB/sï¼Œç”¨è¿™ä¸ªååé‡ç²—ä¼° Pile 825GB æ—¶é—´ï¼š$t \approx \frac{825\times 10^9}{1.5\times 10^6}s\approx 6d$ï¼Œçº¦ $6$ å¤©ã€‚

(d)

```
[tinystories:train] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/tinystories_train.uint16.npy
  tokens=541,229,347  count=52.50s (42.43 MB/s)  write=123.90s (17.98 MB/s)  encode=4368132 tok/s
[tinystories:valid] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/tinystories_valid.uint16.npy
  tokens=5,465,883  count=1.60s (14.07 MB/s)  write=2.67s (8.43 MB/s)  encode=2047747 tok/s
[owt:train] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/owt_train.uint16.npy
  tokens=2,727,120,452  count=487.05s (24.47 MB/s)  write=1130.65s (10.54 MB/s)  encode=2411991 tok/s
[owt:valid] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/owt_valid.uint16.npy
  tokens=66,401,098  count=13.88s (20.89 MB/s)  write=30.31s (9.57 MB/s)  encode=2190728 tok/s
```

é€‰æ‹© uint16 æ˜¯å› ä¸º $[0,65535]$ çš„å€¼åŸŸè¦†ç›– 10K å’Œ 32K å¤§å°çš„ vocab ä¸‹çš„ token IDï¼ŒåŒæ—¶æ¯” uint32 èŠ‚çœä¸€åŠçš„ç©ºé—´ã€‚



## transformer_accounting

(a)

$V$ è¡¨ç¤º vocab_sizeï¼Œ$L$ è¡¨ç¤º num_layersï¼Œ$h$ è¡¨ç¤º num_headsã€‚

å‚æ•°é‡ï¼š

- Embedding $Vd$ï¼›
- LM head $Vd$ï¼›
- æ¯å±‚ attn proj $4d_{model}^2$ï¼ˆ$W_Q,W_K,W_V,W_O$ï¼‰ï¼›
- æ¯å±‚ FFN $3d_{model}d_{ff}$ï¼›
- æ¯å±‚ RMSNorm $2d_{model}$ï¼›
- æœ€ç»ˆ RMSNorm $d_{model}$ï¼›

æ€»å‚æ•° $2Vd_{model}+L(4d_{model}^2+3d_{model}d_{ff}+2d_{model})+d_{model}$ï¼Œå¸¦å…¥ GPT-2 XL çš„æ•°æ®å¾—å‚æ•°é‡çº¦ $2.13B$ï¼Œå•ç²¾åº¦å­˜å‚¨çº¦ $8.5G$ã€‚

(b)

åœ¨ seq_len = 1024 çš„æƒ…å†µä¸‹ï¼ŒçŸ©ä¹˜åœ¨ä»¥ä¸‹åœºæ™¯å‡ºç°ï¼š

- æ¯å±‚ï¼ˆå…± 48 å±‚ï¼‰ï¼š
  1. QKV Projectionï¼š`(1024, 1600) @ (1600, 1600) -> (1024, 1600)` *3ï¼›
  2. Attn scoresï¼š`(25*1024, 64) @ (64, 1024) -> (25*1024, 1024)`ï¼›
  3. Attn weightedï¼š`(25*1024, 1024) @ (1024, 64) -> (25*1024, 64)`ï¼›
  4. Out Projï¼š`(1024, 1600) @ (1600, 1600) -> (1024, 1600)`ï¼›
  5. FFN w1/w3ï¼š`(1024, 1600) @ (1600, 6400) -> (1024, 6400)` *2ï¼›
  6. FFN w2ï¼š`(1024, 6400) @ (6400, 1600) -> (1024, 1600)`ï¼›
- LM headï¼š
  7. Outputï¼š`(1024, 1600) @ (1600, 50257) -> (1024, 50257)`ã€‚

Total FLOPs = 4.513T.

(c)

```
--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      754.975B
Attention scores:     161.061B
Attention weighted:   161.061B
Output projection:    251.658B
FFN (SwiGLU):           3.020T
LM head:              164.682B

Total FLOPs:            4.513T

--- FLOPs Proportions ---
       QKV proj: 16.73%
    Attn scores:  3.57%
  Attn weighted:  3.57%
       Out proj:  5.58%
            FFN: 66.91%
        LM head:  3.65%
```

å› æ­¤ FFN å æ®æœ€å¤šçš„ FLOPsã€‚

(d)

```
================================================================================
GPT-2 Small
================================================================================
Config: vocab=50257, ctx=1024, L=12, d=768, h=12, d_ff=3072

--- Parameters ---
Token embedding:       38.597M
Per layer:
  MHSA:                 2.359M
  FFN (SwiGLU):         7.078M
  RMSNorm (x2):         1.536K
  Layer total:          9.439M
All layers:           113.265M
Final RMSNorm:             768
LM head:               38.597M

Total parameters:     190.460M (190,460,160)
Memory (fp32):      0.7618 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:       43.487B
Attention scores:      19.327B
Attention weighted:    19.327B
Output projection:     14.496B
FFN (SwiGLU):         173.946B
LM head:               79.047B

Total FLOPs:          349.630B

--- FLOPs Proportions ---
       QKV proj: 12.44%
    Attn scores:  5.53%
  Attn weighted:  5.53%
       Out proj:  4.15%
            FFN: 49.75%
        LM head: 22.61%

================================================================================
GPT-2 Medium
================================================================================
Config: vocab=50257, ctx=1024, L=24, d=1024, h=16, d_ff=4096

--- Parameters ---
Token embedding:       51.463M
Per layer:
  MHSA:                 4.194M
  FFN (SwiGLU):        12.583M
  RMSNorm (x2):         2.048K
  Layer total:         16.779M
All layers:           402.702M
Final RMSNorm:          1.024K
LM head:               51.463M

Total parameters:     505.630M (505,629,696)
Memory (fp32):      2.0225 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      154.619B
Attention scores:      51.540B
Attention weighted:    51.540B
Output projection:     51.540B
FFN (SwiGLU):         618.475B
LM head:              105.397B

Total FLOPs:            1.033T

--- FLOPs Proportions ---
       QKV proj: 14.97%
    Attn scores:  4.99%
  Attn weighted:  4.99%
       Out proj:  4.99%
            FFN: 59.87%
        LM head: 10.20%

================================================================================
GPT-2 Large
================================================================================
Config: vocab=50257, ctx=1024, L=36, d=1280, h=20, d_ff=5120

--- Parameters ---
Token embedding:       64.329M
Per layer:
  MHSA:                 6.554M
  FFN (SwiGLU):        19.661M
  RMSNorm (x2):         2.560K
  Layer total:         26.217M
All layers:           943.811M
Final RMSNorm:          1.280K
LM head:               64.329M

Total parameters:       1.072B (1,072,469,760)
Memory (fp32):      4.2899 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      362.388B
Attention scores:      96.637B
Attention weighted:    96.637B
Output projection:    120.796B
FFN (SwiGLU):           1.450T
LM head:              131.746B

Total FLOPs:            2.258T

--- FLOPs Proportions ---
       QKV proj: 16.05%
    Attn scores:  4.28%
  Attn weighted:  4.28%
       Out proj:  5.35%
            FFN: 64.20%
        LM head:  5.84%
```

éšç€æ¨¡å‹å¤§å°æå‡ï¼ŒFFN FLOPs å æ¯”æå‡ï¼ŒLM head FLOPs å æ¯”ä¸‹é™ï¼Œå…¶ä½™éƒ¨åˆ†å˜åŒ–ä¸æ˜æ˜¾ã€‚

(e)

```
================================================================================
GPT-2 XL (16K context)
================================================================================
Config: vocab=50257, ctx=16384, L=48, d=1600, h=25, d_ff=6400

--- Parameters ---
Token embedding:       80.411M
Per layer:
  MHSA:                10.240M
  FFN (SwiGLU):        30.720M
  RMSNorm (x2):         3.200K
  Layer total:         40.963M
All layers:             1.966B
Final RMSNorm:          1.600K
LM head:               80.411M

Total parameters:       2.127B (2,127,057,600)
Memory (fp32):      8.5082 GB

--- FLOPs (batch=1, seq_len=16384) ---
QKV projections:       12.080T
Attention scores:      41.232T
Attention weighted:    41.232T
Output projection:      4.027T
FFN (SwiGLU):          48.318T
LM head:                2.635T

Total FLOPs:          149.523T

--- FLOPs Proportions ---
       QKV proj:  8.08%
    Attn scores: 27.58%
  Attn weighted: 27.58%
       Out proj:  2.69%
            FFN: 32.32%
        LM head:  1.76%
```

Attention FLOPs å æ¯”æå‡å¾ˆå¤§ï¼ŒFFN FLOPs å æ¯”é™ä½ï¼Œå‰©ä¸‹çš„éƒ¨åˆ†å æ¯”ä¹Ÿé™ä½ï¼Œå› ä¸ºéšç€æ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œattention éƒ¨åˆ†çš„è®¡ç®—é‡æ˜¯å¹³æ–¹å¢é•¿çš„ï¼Œå…¶å®ƒçš„åªæ˜¯çº¿æ€§å¢é•¿ã€‚

æ–‡æœ¬é•¿åº¦å¢åŠ  16 å€ï¼ŒFLOPs å¢é•¿ 33.13 å€ã€‚



## learning_rate_tuning

```
Learning Rate: 10.0
Iteration | Loss
-------------------------
        0 | 22.550362
        1 | 14.432232
        2 | 10.638825
        3 | 8.323745
        4 | 6.742233
        5 | 5.590084
        6 | 4.714494
        7 | 4.028669
        8 | 3.479073
        9 | 3.030659

Learning Rate: 100.0
Iteration | Loss
-------------------------
        0 | 22.706253
        1 | 22.706249
        2 | 3.895776
        3 | 0.093235
        4 | 0.000000
        5 | 0.000000
        6 | 0.000000
        7 | 0.000000
        8 | 0.000000
        9 | 0.000000

Learning Rate: 1000.0
Iteration | Loss
-------------------------
        0 | 26.970787
        1 | 9736.452148
        2 | 1681638.750000
        3 | 187064336.000000
        4 | 15152208896.000000
        5 | 956277915648.000000
        6 | 49092185030656.000000
        7 | 2112155662942208.000000
        8 | 77849555005079552.000000
        9 | 2499835618138259456.000000
```

lr = 1e1 æ—¶ loss ç¼“æ…¢ç¨³å®šä¸‹é™ï¼›lr = 1e2 æ—¶ loss è¿…é€Ÿæ”¶æ•›åˆ° 0ï¼›lr = 1e3 æ—¶ loss è¿…é€Ÿå‘æ•£åˆ°å¾ˆå¤§çš„å€¼ã€‚



## adamwAccounting

(a)

æ€»å³°å€¼å†…å­˜ = $256NÂ·dÂ² + 32VÂ·d + 16NÂ·BÂ·LÂ·d + 4NÂ·BÂ·hÂ·LÂ² + 4BÂ·LÂ·V$ å­—èŠ‚ï¼š

- å‚æ•°: $64NÂ·dÂ² + 8VÂ·d$ å­—èŠ‚ = $(16NÂ·dÂ² + 2VÂ·d) Ã— 4$
- æ¢¯åº¦: $64NÂ·dÂ² + 8VÂ·d$ å­—èŠ‚ = $(16NÂ·dÂ² + 2VÂ·d) Ã— 4$
- ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW): $128NÂ·dÂ² + 16VÂ·d$ å­—èŠ‚ = $(16NÂ·dÂ² + 2VÂ·d) Ã— 8$
- æ¿€æ´» (ä¸»è¦é¡¹): $16NÂ·BÂ·LÂ·d + 4NÂ·BÂ·hÂ·LÂ² + 4BÂ·LÂ·V$ å­—èŠ‚
  - å…¶ä¸­ $16NÂ·BÂ·LÂ·d = 4 Ã— N Ã— B Ã— L Ã— d_{ff}$ (FFNä¸­W1/W3è¾“å‡º)
  - $4NÂ·BÂ·hÂ·LÂ² = 4 Ã— N Ã— B Ã— h Ã— LÂ²$ (Attention scores)

å…¶ä¸­ $N$=å±‚æ•°, $d$=æ¨¡å‹ç»´åº¦, $V$=è¯è¡¨å¤§å°, $B$=batch_size, $L$=ä¸Šä¸‹æ–‡é•¿åº¦, $h$=æ³¨æ„åŠ›å¤´æ•°

(b)

å†…å­˜å…¬å¼: $M(B) = 6.051 Ã— B + 31.69$ GB

æœ€å¤§batch_size @ 80GB: $B_{max} = 7$

æ¨å¯¼ï¼š

- GPT-2 XLå‚æ•°æ•°: $2.127B$ (2.127äº¿)
- å›ºå®šå†…å­˜: å‚æ•°(7.92GB) + æ¢¯åº¦(7.92GB) + ä¼˜åŒ–å™¨(15.85GB) = 31.69GB
- æ¯batché¢å¤–å†…å­˜: æ³¨æ„åŠ›åˆ†æ•°(4.688GB) + FFN(1.172GB) + logits(0.192GB) = 6.051GB
- $B_{max} = \lfloor(80-31.69)/6.051\rfloor = 7$

(c)

æ€»FLOPs = $48NÂ·BÂ·LÂ·dÂ² + 6NÂ·BÂ·LÂ²Â·d + 3BÂ·LÂ·VÂ·d + 144NÂ·dÂ² + 18VÂ·d$

ä¸»è¦ç»„æˆï¼š

- å‰å‘ä¼ æ’­: $F_{fwd} = 16NÂ·BÂ·LÂ·dÂ² + 2NÂ·BÂ·LÂ²Â·d + BÂ·LÂ·VÂ·d$
- åå‘ä¼ æ’­: $F_{bwd} = 2 Ã— F_{fwd}$ (æŒ‰è®ºæ–‡å‡è®¾)
- ä¼˜åŒ–å™¨: $F_{opt} = 9 Ã— (16NÂ·dÂ² + 2VÂ·d)$ (9 FLOPs per parameter)

ä¸»å¯¼é¡¹ (å 90%+): $48NÂ·BÂ·LÂ·dÂ²$ â† çŸ©é˜µä¹˜æ³•

(d)

çº¦ 3,292 å¤© (9.0å¹´)

è®¡ç®—ï¼š

- æ¯æ­¥FLOPs: $6.933 Ã— 10^{15}$
- æ€»FLOPs: $2.773 Ã— 10^{21}$
- æœ‰æ•ˆååé‡ (A100 @ 50% MFU): $9.75 TFLOPs/s$
- æ—¶é—´: $2.773 Ã— 10^{21} / (9.75 Ã— 10^{12}) = 284.4M$ ç§’ = 3,292å¤©





## learning_rate

(a)

é€‰æ‹©äº† [1e-3, 3e-3, 5e-3, 8e-3, 1e-2]ï¼ŒåŸºæœ¬ç­‰è·ï¼Œtrain/loss curve å¦‚ä¸‹ï¼š

![img](img1.png)

åŸºæœ¬ç­‰è·åœ°é€‰äº† 1e-3 åˆ° 1e-2 çš„å­¦ä¹ ç‡ï¼Œå‘ç°åˆ° 8e-3 ä¸ºæ­¢çš„å­¦ä¹ ç‡éƒ½æ­£å¸¸çš„æ”¶æ•›åˆ°äº† loss 1.32 å·¦å³ï¼Œä½äº 1.45ï¼›ä½† 1e-2 çš„å­¦ä¹ ç‡ä¸ç¨³å®šï¼Œåå¤è§¦å‘ grad_normï¼Œloss è¡¨ç°ä¹Ÿæ˜æ˜¾å¼‚å¸¸ã€‚

(b) åœ¨è¿™ç»„å®éªŒä¸­ï¼Œæœ€ç»ˆ loss å…³äºå­¦ä¹ ç‡å•è°·ï¼Œæ‰€ä»¥æœ€ä¼˜å­¦ä¹ ç‡åœ¨ 5e-3 å·¦å³ã€‚



## batch_size_experiment

![](img5.png)

åœ¨ lr=6e-4 å›ºå®š ä¸” æ€» token ç›¸åŒ çš„æ¡ä»¶ä¸‹ï¼Œbatch è¶Šå°æœ€ç»ˆ loss è¶Šä½ï¼šbs16 æœ€å¥½ï¼ˆâ‰ˆ1.326ï¼‰ï¼Œéšå bs32ï¼ˆâ‰ˆ1.355ï¼‰ã€bs64ï¼ˆâ‰ˆ1.373ï¼‰ã€bs128 æœ€å·®ï¼ˆâ‰ˆ1.413ï¼‰ã€‚

è§£é‡Šï¼šå° batch çš„æ¢¯åº¦å™ªå£°æ›´å¤§ï¼Œèµ·åˆ°ä¸€å®šâ€œéšå¼æ­£åˆ™åŒ–/æ¢ç´¢â€ä½œç”¨ï¼Œç»™å®šç›¸åŒ token é¢„ç®—æ—¶æ›´å®¹æ˜“æ‰¾åˆ°æ›´å¥½çš„è§£ï¼›è€Œå¤§ batch æ¢¯åº¦æ›´å¹³æ»‘ã€æ›´ç¡®å®šï¼Œä½†åœ¨åŒä¸€å­¦ä¹ ç‡ä¸‹å¾€å¾€éœ€è¦ æ›´å¤§çš„ lrï¼ˆæˆ–ä¸åŒ lr schedule/warmupï¼‰ æ‰èƒ½è¾¾åˆ°åŒç­‰ä¼˜åŒ–æ•ˆç‡ï¼Œå¦åˆ™ä¼šæ”¶æ•›åˆ°æ›´å·®çš„ç‚¹ã€‚

ç»“è®ºï¼šå¤§ batch å¹¶ä¸ä¸€å®šæ›´å·®ï¼Œä½†éœ€è¦é‡æ–°è°ƒå‚ï¼ˆå°¤å…¶ lrï¼‰ï¼›åœ¨å½“å‰ lr ä¸‹ï¼Œå° batch æ›´å ä¼˜ã€‚



## generate

```
temperature=0.95 top_p=0.1 max_new_tokens=256 stop_on_eos=False seed=0
prompt_tokens: 17
total_tokens: 273
new_tokens: 256
---TEXT_START---
Once upon a time, he was so excited to see the surprise. 
When he arrived at the park, he saw a big, colorful slide. He was so excited and he ran to it. He climbed up the slide and slid down with a big smile on his face. 
He had so much fun sliding down the slide. He was so happy that he had found the surprise of the slide. He couldn't wait to go back again!
<|endoftext|>
Once upon a time, there was a little girl named Lily. She had a big, red ball that she loved to play with. One day, she went to the park with her mom and her ball.
At the park, Lily saw a boy named Tim. Tim was sad because he lost his toy. Lily wanted to help Tim find his toy. They looked under the slide, behind the swings, and in the sandbox.
Finally, they found the toy near the swings. Tim was so happy! He said, "Thank you, Lily!" They played together and had lots of fun. From that day on, Lily and Tim became best friends.
<|endoftext|>
Once upon a time, there was a little girl named Lily. She loved to play with her toys and eat yummy food. One day, she found
---TEXT_END---
```

æ–‡å­—å¯è¯»æ€§è¾ƒé«˜ï¼Œç”¨è¯å¤šè§äº TinyStories æ•°æ®é›†ä¸­å‡ºç°çš„ç®€å•å•è¯ï¼Œå•ä¸ªæ•…äº‹é•¿çŸ­ä¹Ÿå¾ˆæ¥è¿‘ï¼›è°ƒé«˜ temperatureã€è°ƒä½ top-p ä¹‹åæ•…äº‹è¿è´¯æ€§æå‡ï¼Œä½†å€¾å‘äºè¾“å‡ºåŸºæœ¬ä¸€æ ·çš„æ•…äº‹ï¼ˆè§‚å¯Ÿç¬¬äºŒæ®µæ•…äº‹çš„å¼€å¤´ï¼‰ã€‚





## layer_norm_ablation

(a)

![](img2.png)

å»æ‰ RMSNorm åï¼Œç”¨ä¹‹å‰çš„æœ€ä¼˜å­¦ä¹ ç‡ï¼ˆ5e-3ï¼‰è®­ç»ƒä¼šç›´æ¥æ•°å€¼å‘æ•£åˆ° NaNï¼›é™ä½å­¦ä¹ ç‡å¯ä»¥æ¢å¤è®­ç»ƒï¼Œä½†éœ€è¦æ˜¾è‘—å‡å°ï¼ˆä¾‹å¦‚ 1e-3 ä»ä¼šå‡ºç°å¤§å¹…éœ‡è¡ï¼Œ5e-4 æ‰ç›¸å¯¹ç¨³å®šï¼‰ã€‚æ€»ä½“ä¸Šï¼ŒRMSNorm æä¾›äº†å°ºåº¦/æ¢¯åº¦çš„ç¨³å®šåŒ–ï¼Œä½¿æ¨¡å‹èƒ½åœ¨æ›´å¤§çš„å­¦ä¹ ç‡ä¸‹ç¨³å®šè®­ç»ƒå¹¶æ›´é¡ºç•…åœ°æ”¶æ•›ï¼›ç§»é™¤åè®­ç»ƒå¯¹å­¦ä¹ ç‡æ›´æ•æ„Ÿã€æ›´å®¹æ˜“å‡ºç° loss çˆ†ç‚¸ï¼Œä¸”åœ¨ç¨³å®šè®¾ç½®ä¸‹å¾€å¾€æ”¶æ•›æ›´æ…¢æˆ–æœ€ç»ˆæ•ˆæœæ›´å·®ã€‚



## no_pos_emb

![](img3.png)

NoPEï¼ˆå»æ‰ RoPE/ä¸åŠ ä½ç½®ç¼–ç ï¼‰ï¼šè®­ç»ƒä»ç„¶ç¨³å®šæ”¶æ•›ï¼Œä½†æœ€ç»ˆ `train/loss_smoothed` æ˜æ˜¾å˜å·®ã€‚å…·ä½“åœ¨ step=9900ï¼šlr=1e-3 æ—¶ 1.4337 vs 1.3679ï¼Œlr=5e-3 æ—¶ 1.388 vs 1.3166ã€‚ç»“è®ºæ˜¯ causal mask è™½èƒ½è®©æ¨¡å‹å­¦åˆ°éƒ¨åˆ†é¡ºåºä¿¡æ¯ï¼Œä½†ç¼ºå°‘æ˜¾å¼ä½ç½®ç¼–ç ä¼šé™ä½ä½ç½®å¯¹é½ä¸é•¿ç¨‹å»ºæ¨¡èƒ½åŠ›ï¼Œå¯¼è‡´æ€§èƒ½ä¸Šé™ä¸‹é™ã€‚



## swiglu_ablation

![](img4.png)

SiLUï¼ˆæŠŠ SwiGLU æ¢æˆ SiLUï¼‰æ¶ˆèç»“æœï¼šè®­ç»ƒè¿‡ç¨‹ç¨³å®šä½†æ€§èƒ½é€€åŒ–ã€‚åœ¨ step=9900ï¼Œ`lr=1e-3` æ—¶ 1.412 vs 1.3679ï¼ˆâ†‘0.044ï¼‰ï¼Œ`lr=5e-3` æ—¶ 1.3438 vs 1.3166ï¼ˆâ†‘0.027ï¼‰ã€‚ç»“è®ºï¼šSwiGLU åœ¨ç›¸åŒè®¡ç®—/è¶…å‚ä¸‹æ›´â€œè¡¨è¾¾åŠ›å¼ºâ€ï¼ˆé—¨æ§å¸¦æ¥æ›´å¥½çš„ç‰¹å¾é€‰æ‹©ä¸éçº¿æ€§ï¼‰ï¼Œè€Œçº¯ SiLU çš„ FFN å®¹é‡ä¸åŠ¨æ€æ€§æ›´å¼±ï¼Œå› æ­¤æœ€ç»ˆ loss æ›´é«˜ã€‚





## main_experiment

![](img6.png)

è®­ç»ƒçš„æ€» token æ•°æ˜¯ tinystories çš„ä¸¤å€ï¼Œç”Ÿæˆçš„æ–‡æœ¬æ•ˆæœä¸ä½³ï¼š

```
================================================================================
OWT Generation
================================================================================
device:      cuda
checkpoint:  checkpoints/owt_checkpoint_final.pt
iteration:   40000
config:      configs/train_openwebtext.json
tokenizer:   cs336_basics/tokenizer_output_owt
temperature=0.9 top_p=0.95 max_new_tokens=256 stop_on_eos=False seed=0 num_samples=4
================================================================================

---SAMPLE 1/4---
prompt_tokens: 22 | total_tokens: 278 | new_tokens: 256
The meaning of life is pÃ¦rnÃ¥re.

. BÃ¥r det fÃ¶rket ider som en gruffÃ¥rst pÃ¥ eventgensningerfÃ¤gstÃ¤.

. VÃ¥ att

. HÃ¤nokalÃ¤t med krystittare pÃ¥ hÃ¥gon sÃ¥ va en gruffÃ¥rng i Ã¶ffÃ¸r stÃ¥rtsÃ¤ndet som bÃ¸gÃ¥rjes. " Eider skatkaden utgyrligÃ¤ samlet har spiktet den. KÃ¶tiger enjedade sande vont Ã¥brot.

Â» Mengen, och betÃ¥r. Eider kvilla at Ã¶ffÃ¸rter sÃ¥ krystÃ¥rtler Fraikkemple della dalle. Sustre sigurden Facebook det net. Maltser av mobli fÃ¶raanskket, vÃ¤r redige relativarna utr, un uppÃ¥r kostas tÃ¤ds med skitÃ¤lla. Gesigen Ã¶ffjen nostar omkslarligdatt, kredskittade

---SAMPLE 2/4---
prompt_tokens: 22 | total_tokens: 278 | new_tokens: 256
The meaning of life isuper ia.

Then, you can look up all the ancient hands on the stone.

In high school,

They will learn English in ancient and read Finnish as the name.

But I like it!

Everywhere I see it and feel other ways that something strange is different from what some people have been saying.

So what are you dreaming about?

Looking down to the tree, we are as

secondary

A lot of people might think they had in the flower.

But do you think that the flower would make the flower?

Nowhere in coffee is this mind of the trees. So, a lot of people have felt an affinity for flowers, in the painting. So, here it is, it's another strange shade.

So, do you ever see a flower?

Gnit must have a flower in the flower. And a smile.

But all of that, when it's on the flower, there is a lot of flowers.

They're just amazing.

And, I said, that's the more people there are.

I just want to go back to the Silvermont.

Here we have some

---SAMPLE 3/4---
prompt_tokens: 22 | total_tokens: 278 | new_tokens: 256
The meaning of life ischrexe: gvstv) etrato nt personet, varefractile, his) dalem dif, his) zÃ , pi, karyu viÅ¡ ie biovsyta kandu ni llt, fjudiÅ¡i ne kanskation,/o ali, sin je njen vada po, prok tnrt i nejpma ÄŒek PÃ©rez

Provor Junctionâ€™s ali, tvljiv liÅ¡tomen lvljiv kanjuju nekvamo, kobese je zelo. Ude menovi predrucer, drova viÅ¡lj. VADAÃc o posti ovu, nekvrstvce. CRC lvljih razu hu zukk pela toke, nei deks uma svijen toke. Kazodomovan.

Ude 2014 morajan narbati. Nada koji njen sevlj. Ijjci zaÅ¾g nogihe, m

---SAMPLE 4/4---
prompt_tokens: 22 | total_tokens: 278 | new_tokens: 256
The meaning of life is ) vi e cedar vje l ka ha iettaem er lang sjon Ê¿nda /svular É»t, dag ka pu ll sdkl.

Tart neptam umaleo vieneale, doble che i glatadmma scrca i juj.

Tart neli ikinausu,o "~" suisenih i mosma apze-juje...

Tart nelia opi ikina sÃ¥ deade ile ha tid ort sen valom er et could Ã¥ mjÃ¶rning om fÃ¥r deveka opposom upp iptam minaj motwe.

Compati tard nell to okls, sÃ¥ lymare i met i samolÃ¥g ess nÃ¤l jakkans i lang, ka ho sowan, svÃ¥ra, ka- sommen uppleh ha fÃ¦r prÃ¤t och finag trÃ¥r mijnfentem i nÃ¥r nÃ¤l gÃ¥r har vÃ¥ra nÃ¥
```

å°½ç®¡æ€»è®­ç»ƒ token è¾¾åˆ° TinyStories çš„ä¸¤å€ï¼Œä½†åœ¨æ›´å¤æ‚ã€æ›´é«˜ç†µçš„ OpenWebText ä¸Šï¼ŒåŒç­‰æ¨¡å‹è§„æ¨¡ä¸è®­ç»ƒè®¾ç½®ä¸‹ä»å¯èƒ½æ¬ è®­ç»ƒï¼Œå¯¼è‡´ç”Ÿæˆç»“æœå‡ºç°å¤šè¯­æ··æ‚ä¸ä¹±ç ã€‚é«˜æ¸©é‡‡æ ·ï¼ˆtemperature=0.9, top_p=0.95ï¼‰ä¼šè¿›ä¸€æ­¥æ”¾å¤§æ¨¡å‹ä¸æˆç†Ÿæ—¶å¯¹å°¾éƒ¨ç¨€æœ‰ token çš„é‡‡æ ·ï¼Œä»è€Œæ¶åŒ–å¯è¯»æ€§ï¼›æ­¤å¤–ï¼ŒOWT çš„å™ªå£°ä¸ tokenizer/æ¸…æ´—ç­–ç•¥ä¹Ÿä¼šæ˜¾è‘—å½±å“ç”Ÿæˆè´¨é‡ã€‚å»ºè®®ç”¨æ›´ä¿å®ˆçš„é‡‡æ ·ï¼ˆæ›´ä½ temperature/top_p æˆ– greedyï¼‰éªŒè¯æ¨¡å‹èƒ½åŠ›ï¼Œå¹¶åœ¨å¿…è¦æ—¶å¢åŠ è®­ç»ƒé¢„ç®—/è°ƒæ•´å­¦ä¹ ç‡ä¸æ•°æ®æ¸…æ´—æ¥æ”¹å–„ç”Ÿæˆã€‚