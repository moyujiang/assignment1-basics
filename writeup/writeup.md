## Unicode1

(a) ç©ºä¸² `""`ï¼›

(b) `__str__(chr(0))` ç»™å‡ºç©ºä¸²ï¼Œ`__repr__(chr(0))` ç»™å‡º `'\x00'`ï¼Œå³ç”¨å¼•å·åŒ…è£¹çš„ unicodeï¼›

(c) 

```py
>>> chr(0)
'\x00'
>>> print(chr(0))

>>> "this is a test" + chr(0) + "string"
'this is a test\x00string'
>>> print("this is a test" + chr(0) + "string")
this is a teststring
```



## Unicode2

(a) å¹³å‡ä¿¡æ¯å¯†åº¦é«˜ï¼ˆç¼–ç å­—æ¯åªéœ€è¦ $1$ å­—èŠ‚ï¼‰ï¼Œç”¨ 00 å¡«å……çš„æƒ…å†µæ›´å°‘ï¼Œç¬¦åˆäº’è”ç½‘æ–‡æœ¬å­˜å‚¨æ ¼å¼ï¼›

(b) `b'\xf0\x9f\xa4\x93'`ï¼Œemoji ğŸ¤“ çš„ Unicodeã€‚UTF-8 æ˜¯å˜é•¿çš„ï¼Œä»»ä½•åœ¨ UTF-8 ä¸‹ç¼–ç è¶…è¿‡ä¸€å­—èŠ‚çš„éƒ½ä¼šåœ¨ `.decode` å¼€å¤´å­—èŠ‚çš„æ—¶å€™ç›´æ¥æŠ¥é”™ã€‚

(c) `b'\xf0\x9f`ï¼Œ`0xf0` è¯´æ˜æ¥ä¸‹æ¥è¿˜ä¼šæœ‰ 3 byte æ¥ä¸€èµ·ç¼–ç ä¸€ä¸ªå­—ç¬¦ï¼Œä½†æ¥ä¸‹æ¥åªæœ‰ 1 byte äº†ï¼Œè§£ç æ—¶æŠ¥é”™ã€‚



## train_bpe_tinystories

(a) 

```
Time taken: 88.10 seconds (0.0245 hours)
Peak memory usage: 2.31 GB
Vocabulary size: 10000
Number of merges: 9743
Longest token: b' accomplishment' (15 bytes)
As string: ' accomplishment'
```

(b) åœ¨ valid-set ä¸Šï¼Œpretokenize èŠ±è´¹ 0.74sï¼Œtrain èŠ±è´¹ 3.67sï¼›åœ¨ train-set ä¸Šï¼Œpretokenize èŠ±è´¹ 74.03sï¼Œ train èŠ±è´¹ 13.57sã€‚



## train_bpe_expts_owt

(a)

```
Time taken: 12932.71 seconds (3.5924 hours)
Pretokenize time: 525.41 seconds
Train time: 8944.50 seconds
Peak memory usage: 28.58 GB
Vocabulary size: 32000
Number of merges: 31743

Longest token: b'\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82\xc3\x83\xc3\x82' (64 bytes)
As string: 'ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚'
```

(b)

ç½‘ç»œæ•°æ®æ›´éš¾æ¸…æ´—ï¼Œå¯¼è‡´ owt ä¸Šçš„æœ€é•¿ token ä¸æ˜¯æœ‰æ„ä¹‰çš„å•è¯ï¼›owt æ•°æ®é›†æ˜¾è‘—å¤§äº TinyStoriesï¼Œè¿™å¯¼è‡´ merge é˜¶æ®µæ…¢å¾ˆå¤šã€‚



## tokenizer_experiments

(a)

TinyStories tokenizer (10K) compression ratio (bytes/token): 4.0908
OpenWebText tokenizer (32K) compression ratio (bytes/token): 4.5357

(b)

OpenWebText sample with TinyStories tokenizer (bytes/token): 3.3438
OWT token count multiplier (TinyStories vs OWT tokenizer): 1.356x
OWT bytes/token change vs OWT tokenizer: -26.28%

TinyStories ä¸­å¤šç®€çŸ­æ•…äº‹ï¼Œç”¨è¯ç›¸å¯¹ç®€å•ï¼Œä¸” vocab æ›´å°ï¼Œå¯¹ owt ä¸­çš„å°‘è§è¯æ±‡ã€URL æ¨¡å¼ç­‰ä¸èƒ½å¾ˆå¥½åœ°åŒ¹é…ï¼Œè¢«åˆ‡ä¸ºæ›´å¤šçš„ tokensï¼Œå‹ç¼©ç‡æ˜¾è‘—ä¸‹é™ã€‚

(c)

OWT tokenizer åœ¨ OWT ä¸Šçº¦ ~1.5 MB/sï¼Œç”¨è¿™ä¸ªååé‡ç²—ä¼° Pile 825GB æ—¶é—´ï¼š$t \approx \frac{825\times 10^9}{1.5\times 10^6}s\approx 6d$ï¼Œçº¦ $6$ å¤©ã€‚

(d)

```
[tinystories:train] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/tinystories_train.uint16.npy
  tokens=541,229,347  count=52.50s (42.43 MB/s)  write=123.90s (17.98 MB/s)  encode=4368132 tok/s
[tinystories:valid] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/tinystories_valid.uint16.npy
  tokens=5,465,883  count=1.60s (14.07 MB/s)  write=2.67s (8.43 MB/s)  encode=2047747 tok/s
[owt:train] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/owt_train.uint16.npy
  tokens=2,727,120,452  count=487.05s (24.47 MB/s)  write=1130.65s (10.54 MB/s)  encode=2411991 tok/s
[owt:valid] -> /Users/moyujiang/Desktop/CS336/assignment1-basics/data/tokenized/owt_valid.uint16.npy
  tokens=66,401,098  count=13.88s (20.89 MB/s)  write=30.31s (9.57 MB/s)  encode=2190728 tok/s
```

é€‰æ‹© uint16 æ˜¯å› ä¸º $[0,65535]$ çš„å€¼åŸŸè¦†ç›– 10K å’Œ 32K å¤§å°çš„ vocab ä¸‹çš„ token IDï¼ŒåŒæ—¶æ¯” uint32 èŠ‚çœä¸€åŠçš„ç©ºé—´ã€‚



## transformer_accounting

(a)

$V$ è¡¨ç¤º vocab_sizeï¼Œ$L$ è¡¨ç¤º num_layersï¼Œ$h$ è¡¨ç¤º num_headsã€‚

å‚æ•°é‡ï¼š

- Embedding $Vd$ï¼›
- LM head $Vd$ï¼›
- æ¯å±‚ attn proj $4d_{model}^2$ï¼ˆ$W_Q,W_K,W_V,W_O$ï¼‰ï¼›
- æ¯å±‚ FFN $3d_{model}d_{ff}$ï¼›
- æ¯å±‚ RMSNorm $2d_{model}$ï¼›
- æœ€ç»ˆ RMSNorm $d_{model}$ï¼›

æ€»å‚æ•° $2Vd_{model}+L(4d_{model}^2+3d_{model}d_{ff}+2d_{model})+d_{model}$ï¼Œå¸¦å…¥ GPT-2 XL çš„æ•°æ®å¾—å‚æ•°é‡çº¦ $2.13B$ï¼Œå•ç²¾åº¦å­˜å‚¨çº¦ $8.5G$ã€‚

(b)

åœ¨ seq_len = 1024 çš„æƒ…å†µä¸‹ï¼ŒçŸ©ä¹˜åœ¨ä»¥ä¸‹åœºæ™¯å‡ºç°ï¼š

- æ¯å±‚ï¼ˆå…± 48 å±‚ï¼‰ï¼š
  1. QKV Projectionï¼š`(1024, 1600) @ (1600, 1600) -> (1024, 1600)` *3ï¼›
  2. Attn scoresï¼š`(25*1024, 64) @ (64, 1024) -> (25*1024, 1024)`ï¼›
  3. Attn weightedï¼š`(25*1024, 1024) @ (1024, 64) -> (25*1024, 64)`ï¼›
  4. Out Projï¼š`(1024, 1600) @ (1600, 1600) -> (1024, 1600)`ï¼›
  5. FFN w1/w3ï¼š`(1024, 1600) @ (1600, 6400) -> (1024, 6400)` *2ï¼›
  6. FFN w2ï¼š`(1024, 6400) @ (6400, 1600) -> (1024, 1600)`ï¼›
- LM headï¼š
  7. Outputï¼š`(1024, 1600) @ (1600, 50257) -> (1024, 50257)`ã€‚

Total FLOPs = 4.513T.

(c)

```
--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      754.975B
Attention scores:     161.061B
Attention weighted:   161.061B
Output projection:    251.658B
FFN (SwiGLU):           3.020T
LM head:              164.682B

Total FLOPs:            4.513T

--- FLOPs Proportions ---
       QKV proj: 16.73%
    Attn scores:  3.57%
  Attn weighted:  3.57%
       Out proj:  5.58%
            FFN: 66.91%
        LM head:  3.65%
```

å› æ­¤ FFN å æ®æœ€å¤šçš„ FLOPsã€‚

(d)

```
================================================================================
GPT-2 Small
================================================================================
Config: vocab=50257, ctx=1024, L=12, d=768, h=12, d_ff=3072

--- Parameters ---
Token embedding:       38.597M
Per layer:
  MHSA:                 2.359M
  FFN (SwiGLU):         7.078M
  RMSNorm (x2):         1.536K
  Layer total:          9.439M
All layers:           113.265M
Final RMSNorm:             768
LM head:               38.597M

Total parameters:     190.460M (190,460,160)
Memory (fp32):      0.7618 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:       43.487B
Attention scores:      19.327B
Attention weighted:    19.327B
Output projection:     14.496B
FFN (SwiGLU):         173.946B
LM head:               79.047B

Total FLOPs:          349.630B

--- FLOPs Proportions ---
       QKV proj: 12.44%
    Attn scores:  5.53%
  Attn weighted:  5.53%
       Out proj:  4.15%
            FFN: 49.75%
        LM head: 22.61%

================================================================================
GPT-2 Medium
================================================================================
Config: vocab=50257, ctx=1024, L=24, d=1024, h=16, d_ff=4096

--- Parameters ---
Token embedding:       51.463M
Per layer:
  MHSA:                 4.194M
  FFN (SwiGLU):        12.583M
  RMSNorm (x2):         2.048K
  Layer total:         16.779M
All layers:           402.702M
Final RMSNorm:          1.024K
LM head:               51.463M

Total parameters:     505.630M (505,629,696)
Memory (fp32):      2.0225 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      154.619B
Attention scores:      51.540B
Attention weighted:    51.540B
Output projection:     51.540B
FFN (SwiGLU):         618.475B
LM head:              105.397B

Total FLOPs:            1.033T

--- FLOPs Proportions ---
       QKV proj: 14.97%
    Attn scores:  4.99%
  Attn weighted:  4.99%
       Out proj:  4.99%
            FFN: 59.87%
        LM head: 10.20%

================================================================================
GPT-2 Large
================================================================================
Config: vocab=50257, ctx=1024, L=36, d=1280, h=20, d_ff=5120

--- Parameters ---
Token embedding:       64.329M
Per layer:
  MHSA:                 6.554M
  FFN (SwiGLU):        19.661M
  RMSNorm (x2):         2.560K
  Layer total:         26.217M
All layers:           943.811M
Final RMSNorm:          1.280K
LM head:               64.329M

Total parameters:       1.072B (1,072,469,760)
Memory (fp32):      4.2899 GB

--- FLOPs (batch=1, seq_len=1024) ---
QKV projections:      362.388B
Attention scores:      96.637B
Attention weighted:    96.637B
Output projection:    120.796B
FFN (SwiGLU):           1.450T
LM head:              131.746B

Total FLOPs:            2.258T

--- FLOPs Proportions ---
       QKV proj: 16.05%
    Attn scores:  4.28%
  Attn weighted:  4.28%
       Out proj:  5.35%
            FFN: 64.20%
        LM head:  5.84%
```

éšç€æ¨¡å‹å¤§å°æå‡ï¼ŒFFN FLOPs å æ¯”æå‡ï¼ŒLM head FLOPs å æ¯”ä¸‹é™ï¼Œå…¶ä½™éƒ¨åˆ†å˜åŒ–ä¸æ˜æ˜¾ã€‚

(e)

```
================================================================================
GPT-2 XL (16K context)
================================================================================
Config: vocab=50257, ctx=16384, L=48, d=1600, h=25, d_ff=6400

--- Parameters ---
Token embedding:       80.411M
Per layer:
  MHSA:                10.240M
  FFN (SwiGLU):        30.720M
  RMSNorm (x2):         3.200K
  Layer total:         40.963M
All layers:             1.966B
Final RMSNorm:          1.600K
LM head:               80.411M

Total parameters:       2.127B (2,127,057,600)
Memory (fp32):      8.5082 GB

--- FLOPs (batch=1, seq_len=16384) ---
QKV projections:       12.080T
Attention scores:      41.232T
Attention weighted:    41.232T
Output projection:      4.027T
FFN (SwiGLU):          48.318T
LM head:                2.635T

Total FLOPs:          149.523T

--- FLOPs Proportions ---
       QKV proj:  8.08%
    Attn scores: 27.58%
  Attn weighted: 27.58%
       Out proj:  2.69%
            FFN: 32.32%
        LM head:  1.76%
```

Attention FLOPs å æ¯”æå‡å¾ˆå¤§ï¼ŒFFN FLOPs å æ¯”é™ä½ï¼Œå‰©ä¸‹çš„éƒ¨åˆ†å æ¯”ä¹Ÿé™ä½ï¼Œå› ä¸ºéšç€æ–‡æœ¬é•¿åº¦å¢åŠ ï¼Œattention éƒ¨åˆ†çš„è®¡ç®—é‡æ˜¯å¹³æ–¹å¢é•¿çš„ï¼Œå…¶å®ƒçš„åªæ˜¯çº¿æ€§å¢é•¿ã€‚

æ–‡æœ¬é•¿åº¦å¢åŠ  16 å€ï¼ŒFLOPs å¢é•¿ 33.13 å€ã€‚



## learning_rate_tuning

```
Learning Rate: 10.0
Iteration | Loss
-------------------------
        0 | 22.550362
        1 | 14.432232
        2 | 10.638825
        3 | 8.323745
        4 | 6.742233
        5 | 5.590084
        6 | 4.714494
        7 | 4.028669
        8 | 3.479073
        9 | 3.030659

Learning Rate: 100.0
Iteration | Loss
-------------------------
        0 | 22.706253
        1 | 22.706249
        2 | 3.895776
        3 | 0.093235
        4 | 0.000000
        5 | 0.000000
        6 | 0.000000
        7 | 0.000000
        8 | 0.000000
        9 | 0.000000

Learning Rate: 1000.0
Iteration | Loss
-------------------------
        0 | 26.970787
        1 | 9736.452148
        2 | 1681638.750000
        3 | 187064336.000000
        4 | 15152208896.000000
        5 | 956277915648.000000
        6 | 49092185030656.000000
        7 | 2112155662942208.000000
        8 | 77849555005079552.000000
        9 | 2499835618138259456.000000
```

lr = 1e1 æ—¶ loss ç¼“æ…¢ç¨³å®šä¸‹é™ï¼›lr = 1e2 æ—¶ loss è¿…é€Ÿæ”¶æ•›åˆ° 0ï¼›lr = 1e3 æ—¶ loss è¿…é€Ÿå‘æ•£åˆ°å¾ˆå¤§çš„å€¼ã€‚



## adamwAccounting

(a)

æ€»å³°å€¼å†…å­˜ = $256NÂ·dÂ² + 32VÂ·d + 16NÂ·BÂ·LÂ·d + 4NÂ·BÂ·hÂ·LÂ² + 4BÂ·LÂ·V$ å­—èŠ‚ï¼š

- å‚æ•°: $64NÂ·dÂ² + 8VÂ·d$ å­—èŠ‚ = $(16NÂ·dÂ² + 2VÂ·d) Ã— 4$
- æ¢¯åº¦: $64NÂ·dÂ² + 8VÂ·d$ å­—èŠ‚ = $(16NÂ·dÂ² + 2VÂ·d) Ã— 4$
- ä¼˜åŒ–å™¨çŠ¶æ€ (AdamW): $128NÂ·dÂ² + 16VÂ·d$ å­—èŠ‚ = $(16NÂ·dÂ² + 2VÂ·d) Ã— 8$
- æ¿€æ´» (ä¸»è¦é¡¹): $16NÂ·BÂ·LÂ·d + 4NÂ·BÂ·hÂ·LÂ² + 4BÂ·LÂ·V$ å­—èŠ‚
  - å…¶ä¸­ $16NÂ·BÂ·LÂ·d = 4 Ã— N Ã— B Ã— L Ã— d_{ff}$ (FFNä¸­W1/W3è¾“å‡º)
  - $4NÂ·BÂ·hÂ·LÂ² = 4 Ã— N Ã— B Ã— h Ã— LÂ²$ (Attention scores)

å…¶ä¸­ $N$=å±‚æ•°, $d$=æ¨¡å‹ç»´åº¦, $V$=è¯è¡¨å¤§å°, $B$=batch_size, $L$=ä¸Šä¸‹æ–‡é•¿åº¦, $h$=æ³¨æ„åŠ›å¤´æ•°

(b)

å†…å­˜å…¬å¼: $M(B) = 6.051 Ã— B + 31.69$ GB

æœ€å¤§batch_size @ 80GB: $B_{max} = 7$

æ¨å¯¼ï¼š

- GPT-2 XLå‚æ•°æ•°: $2.127B$ (2.127äº¿)
- å›ºå®šå†…å­˜: å‚æ•°(7.92GB) + æ¢¯åº¦(7.92GB) + ä¼˜åŒ–å™¨(15.85GB) = 31.69GB
- æ¯batché¢å¤–å†…å­˜: æ³¨æ„åŠ›åˆ†æ•°(4.688GB) + FFN(1.172GB) + logits(0.192GB) = 6.051GB
- $B_{max} = \lfloor(80-31.69)/6.051\rfloor = 7$

(c)

æ€»FLOPs = $48NÂ·BÂ·LÂ·dÂ² + 6NÂ·BÂ·LÂ²Â·d + 3BÂ·LÂ·VÂ·d + 144NÂ·dÂ² + 18VÂ·d$

ä¸»è¦ç»„æˆï¼š

- å‰å‘ä¼ æ’­: $F_{fwd} = 16NÂ·BÂ·LÂ·dÂ² + 2NÂ·BÂ·LÂ²Â·d + BÂ·LÂ·VÂ·d$
- åå‘ä¼ æ’­: $F_{bwd} = 2 Ã— F_{fwd}$ (æŒ‰è®ºæ–‡å‡è®¾)
- ä¼˜åŒ–å™¨: $F_{opt} = 9 Ã— (16NÂ·dÂ² + 2VÂ·d)$ (9 FLOPs per parameter)

ä¸»å¯¼é¡¹ (å 90%+): $48NÂ·BÂ·LÂ·dÂ²$ â† çŸ©é˜µä¹˜æ³•

(d)

çº¦ 3,292 å¤© (9.0å¹´)

è®¡ç®—ï¼š

- æ¯æ­¥FLOPs: $6.933 Ã— 10^{15}$
- æ€»FLOPs: $2.773 Ã— 10^{21}$
- æœ‰æ•ˆååé‡ (A100 @ 50% MFU): $9.75 TFLOPs/s$
- æ—¶é—´: $2.773 Ã— 10^{21} / (9.75 Ã— 10^{12}) = 284.4M$ ç§’ = 3,292å¤©





## learning_rate

(a)

é€‰æ‹©äº† [1e-3, 3e-3, 5e-3, 8e-3, 1e-2]ï¼ŒåŸºæœ¬ç­‰è·ï¼Œtrain/loss curve å¦‚ä¸‹ï¼š

![img](img1.png)

åŸºæœ¬ç­‰è·åœ°é€‰äº† 1e-3 åˆ° 1e-2 çš„å­¦ä¹ ç‡ï¼Œå‘ç°åˆ° 8e-3 ä¸ºæ­¢çš„å­¦ä¹ ç‡éƒ½æ­£å¸¸çš„æ”¶æ•›åˆ°äº† loss 1.32 å·¦å³ï¼Œä½äº 1.45ï¼›ä½† 1e-2 çš„å­¦ä¹ ç‡ä¸ç¨³å®šï¼Œåå¤è§¦å‘ grad_normï¼Œloss è¡¨ç°ä¹Ÿæ˜æ˜¾å¼‚å¸¸ã€‚

(b) åœ¨è¿™ç»„å®éªŒä¸­ï¼Œæœ€ç»ˆ loss å…³äºå­¦ä¹ ç‡å•è°·ï¼Œæ‰€ä»¥æœ€ä¼˜å­¦ä¹ ç‡åœ¨ 5e-3 å·¦å³ã€‚



